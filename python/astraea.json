{
    "id": "global_reward_additive_action_multi_flow_experiment",
    "world": "world.json",
    "eval_world": "eval_world.json",
    "global" : true,
    "device": "/cpu",
    "PER": false,
    "single_actor_eval": false,
    "LOSS_TYPE": "HUBER",
    "simulator": true,
    "use_normalizer": false,
    "h1_shape": 256,
    "h2_shape": 128,
    "recurrent": true,
    "rec_dim": 5,
    "noise_type": 0,
    "jump_exp": 1500000, 
    "noise_exp": 1500000,
    "train_exp" : 100000,
    "stddev": 1,
    "eval_frequency": 500,
    "state_history": 5,
    "remote": true,
    "max_epochs": 1000,
    "update_delay" : 1000,
    "training_step" : 20,
    "batch_size": 192,
    "policy_delay" : 20,
    "lr_a": 0.00005,
    "lr_c": 0.001,
    "tau": 0.005,
    "gamma": 0.98,
    "memsize": 400000,
    "tb_interval": 1,
    "ckptdir": "ckpt_dir",
    "logdir": "train_dir",
    "eval_dir": "eval_dir",
    "device": "cpu",
    "dequeue_length": 100,
    "num_actors": 2,
    "evaluator_ip": "127.0.0.1:33329",
    "learner_ip": "127.0.0.1:33330",
    "actor_ip": [
        "127.0.0.1:33331",
        "127.0.0.1:33332"
    ]

}